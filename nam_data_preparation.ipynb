{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import gc\n",
    "from os import path\n",
    "import requests\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locations corresponding to each of the NAM forecasts (140x140x2)\n",
    "# nam_locations = np.load(\"nam_locations.npy\")\n",
    "\n",
    "# Latitude and longitude are in the opposite order from the coordinate data for the wind farms\n",
    "# This flips them so that everything is matching\n",
    "# nam_locations = nam_locations[:,:,::-1]\n",
    "\n",
    "# np.save(\"nam_locations_flipped.npy\", nam_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 32 shards of the NAM weather forecasting model output (24049x140x140x9)\n",
    "# Each entry covers the entire 140x140 grid, indicating forecasted value\n",
    "# for 9 weather variables in each time stamp\n",
    "# The meaning of the 9 variables that NAM predicts:\n",
    "# NAM_FIELDS = [\n",
    "#     \"accum_precip\",\n",
    "#     \"dewpoint_temperature_2m\",\n",
    "#     \"downward_short_wave_flux\",\n",
    "#     \"relative_humidity_2m\",\n",
    "#     \"sea_level_pressure\",\n",
    "#     \"temperature_2m\",\n",
    "#     \"total_cloud_cover\",\n",
    "#     \"u_wind_80m\",\n",
    "#     \"v_wind_80m\",\n",
    "# ]\n",
    "\n",
    "all_nam_data = []\n",
    "for i in range(32):\n",
    "    data_file_name = \"nam_data-000\" + \"{:02d}\".format(i) + \"-of-00032.npy\"\n",
    "    if not path.exists(\"nam_data/\" + data_file_name):\n",
    "        # Download the part file\n",
    "        r = requests.get(\"http://storage.googleapis.com/gridmatic/roscoe/\" + data_file_name, stream=True)\n",
    "        with open(\"nam_data/\" + data_file_name, 'wb') as fin:\n",
    "            shutil.copyfileobj(r.raw, fin)\n",
    "            fin.close()\n",
    "    all_nam_data.extend(np.load(\"nam_data/\" + data_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A one-dimensional list of the timestamps for the NAM inputs (24049x1)\n",
    "# The dimension is the number of 1 HOUR INTERVALS in the time period\n",
    "nam_timestamps = pd.read_pickle(\"nam_timestamps.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Pandas dataframe of the timestamps\n",
    "nam_df = pd.DataFrame({'timestamps': nam_timestamps, 'data_objects': all_nam_data});\n",
    "nam_df = nam_df.set_index('timestamps');\n",
    "del nam_timestamps\n",
    "del all_nam_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NAM has a lot of hours/dates missing\n",
    "# We initially thought about cutting the ERCOT down to match, but decided to take advantage of the full NAM dataset\n",
    "# We therefore generate new NAM data so that it more closely matches the data from ERCOT\n",
    "# We generate hourly data, as a middle ground between days of missing NAM data and the 15-minute granularity of ERCOT\n",
    "\n",
    "# OLD SOLUTION WITH PADDING: Just copy one value into all subsequent consecutive hours that are missing\n",
    "# nam_padded_df = nam_df.resample('H').pad();\n",
    "# Now we just do linear interpolation, so this can be ignored\n",
    "# But this is useful to keep as a reference to an earlier technique\n",
    "\n",
    "# Create entries for the missing hours (initially with a value of NaN)\n",
    "nam_unfilled_df = nam_df.resample('H').asfreq();\n",
    "del nam_df\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def find_closest_value(adj_time, direction):\n",
    "    i = 0\n",
    "    adj_value = nam_unfilled_df.iloc[nam_unfilled_df.index.get_loc(adj_time)].iloc[0]\n",
    "    while (type(adj_value) is float and math.isnan(adj_value)):\n",
    "        i += 1\n",
    "        adj_time += direction * pd.Timedelta(hours=1)        \n",
    "        adj_value = nam_unfilled_df.iloc[nam_unfilled_df.index.get_loc(adj_time)].iloc[0]\n",
    "    return (adj_value,i)\n",
    "\n",
    "def generate_new_value(timestamp):\n",
    "    prev_value, prev_dist = find_closest_value(timestamp, -1)\n",
    "    next_value, next_dist = find_closest_value(timestamp, 1)\n",
    "    percentage = 1 if (prev_dist + next_dist) == 0 else prev_dist / (prev_dist + next_dist)\n",
    "    return np.add(prev_value, (np.subtract(next_value, prev_value) * percentage))\n",
    "\n",
    "def fill_in_missing_time(slice_of_df):\n",
    "    value = slice_of_df.iloc[0]\n",
    "    timestamp = slice_of_df.name\n",
    "    timestamp = pd.Timestamp(timestamp) if isinstance(timestamp, np.datetime64) else timestamp\n",
    "    new_val = generate_new_value(timestamp)\n",
    "    slice_of_df.iloc[0] = new_val\n",
    "    return slice_of_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the new data to fill the NaN values\n",
    "nam_interpolated_df = nam_unfilled_df.apply(fill_in_missing_time, axis=1)\n",
    "del nam_unfilled_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the final NAM dataset, with generated data\n",
    "# pprint(nam_interpolated_df[:24])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the NAM data into training and testing\n",
    "# split_df = nam_interpolated_df.copy(deep=True)\n",
    "# split_df = nam_interpolated_df\n",
    "nam_interpolated_df['numerical_index'] = range(0, len(nam_interpolated_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nam_training_df = nam_interpolated_df[nam_interpolated_df['numerical_index'] % 25 != 0]\n",
    "nam_testing_df = nam_interpolated_df[nam_interpolated_df['numerical_index'] % 25 == 0]\n",
    "del nam_interpolated_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the numerical_index\n",
    "nam_training_df = nam_training_df.drop('numerical_index', 1)\n",
    "nam_testing_df = nam_testing_df.drop('numerical_index', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                data_objects\n",
      "timestamps                                                                  \n",
      "2015-01-01 00:00:00-06:00  [[[0.0, 290.7572, 0.0, 75.0, 101402.0, 295.582...\n",
      "2015-01-02 01:00:00-06:00  [[[0.0, 290.68527, 0.0, 77.0, 101398.0, 294.99...\n",
      "2015-01-03 02:00:00-06:00  [[[0.0, 285.07336, 0.0, 59.0, 101692.0, 293.30...\n",
      "2015-01-04 03:00:00-06:00  [[[0.0, 287.89383, 0.0, 68.0, 101669.0, 294.14...\n",
      "2015-01-05 04:00:00-06:00  [[[0.0, 288.7345, 0.0, 66.0, 101515.0, 295.621...\n",
      "2015-01-06 05:00:00-06:00  [[[0.0, 292.64038, 0.0, 80.0, 101687.0, 296.63...\n",
      "2015-01-07 06:00:00-06:00  [[[0.0, 290.5164, 0.0, 76.0, 101596.0, 295.190...\n",
      "2015-01-08 07:00:00-06:00  [[[0.0, 292.16656, 0.0, 81.0, 101441.0, 295.68...\n",
      "2015-01-09 08:00:00-06:00  [[[0.25, 290.54825, 2.375, 73.0, 101470.0, 295...\n",
      "2015-01-10 09:00:00-06:00  [[[0.0, 289.55475, 194.625, 70.0, 101595.0, 29...\n",
      "2015-01-11 10:00:00-06:00  [[[0.0, 290.60452, 405.25, 76.0, 101868.0, 295...\n",
      "2015-01-12 11:00:00-06:00  [[[0.0, 290.88354, 579.25, 76.0, 101848.0, 295...\n",
      "2015-01-13 12:00:00-06:00  [[[0.0, 291.1948, 631.625, 80.0, 101677.0, 294...\n",
      "2015-01-14 13:00:00-06:00  [[[0.0, 289.48532, 273.5417, 72.333336, 101641...\n",
      "2015-01-15 14:00:00-06:00  [[[0.0, 290.7212, 500.5417, 78.0, 101657.33, 2...\n",
      "2015-01-16 15:00:00-06:00  [[[0.0, 291.2889, 325.75, 80.0, 101715.0, 294....\n",
      "2015-01-17 16:00:00-06:00  [[[0.0, 292.0277, 361.5833, 83.0, 101743.0, 29...\n",
      "2015-01-18 17:00:00-06:00  [[[0.0, 291.4997, 276.0833, 83.33333, 101629.0...\n",
      "2015-01-19 18:00:00-06:00  [[[0.0, 292.33533, 99.0, 88.0, 101390.0, 294.6...\n",
      "2015-01-20 19:00:00-06:00  [[[0.0, 291.4914, 65.75, 82.333336, 101277.336...\n",
      "2015-01-21 20:00:00-06:00  [[[0.0, 290.62106, 33.208332, 75.0, 101325.33,...\n",
      "2015-01-22 21:00:00-06:00  [[[0.0, 290.00232, 0.0, 76.0, 101567.0, 294.43...\n",
      "2015-01-23 22:00:00-06:00  [[[0.0, 288.73892, 0.0, 68.333336, 101459.664,...\n",
      "2015-01-24 23:00:00-06:00  [[[0.0, 289.8891, 0.0, 72.66667, 101442.0, 295...\n",
      "2015-01-26 00:00:00-06:00  [[[0.0, 291.5382, 0.0, 84.0, 101632.0, 294.375...\n"
     ]
    }
   ],
   "source": [
    "# Print the training and testing datasets out, to ensure that it still has values after we delete nam_interpolated_df\n",
    "pprint(nam_testing_df[:25])\n",
    "# pprint(nam_training_df[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the split training and testing data to disk\n",
    "cleaned_data = \"cleaned_data/\"\n",
    "training_data_filename = \"nam_training_\"\n",
    "testing_data_filename = \"nam_testing_\"\n",
    "extension = \".pkl.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out the testing data into parts\n",
    "part_length = nam_testing_df.shape[0] // 32\n",
    "# print(part_length)\n",
    "for i in range(32):\n",
    "    testing_data_part = nam_testing_df.iloc[part_length*i:] if (i == 31) else nam_testing_df.iloc[part_length*i:part_length*(i + 1)]\n",
    "    testing_data_part.to_pickle(cleaned_data + testing_data_filename + str(i) + extension, compression='gzip')\n",
    "    del testing_data_part\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_length = nam_training_df.shape[0] // 32\n",
    "for i in range(32):\n",
    "    training_data_part = nam_training_df.iloc[part_length*i:] if i == 31 else nam_training_df.iloc[part_length*i:part_length*(i + 1)]\n",
    "    training_data_part.to_pickle(cleaned_data + training_data_filename + str(i) + extension, compression='gzip')\n",
    "    del training_data_part\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
