{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locations corresponding to each of the NAM forecasts (140x140x2)\n",
    "nam_locations = np.load(\"nam_locations.npy\")\n",
    "\n",
    "# Latitude and longitude are in the opposite order from the coordinate data for the wind farms\n",
    "# This flips them so that everything is matching\n",
    "nam_locations = nam_locations[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "# The 32 shards of the NAM weather forecasting model output (24049x140x140x9)\n",
    "# Each entry covers the entire 140x140 grid, indicating forecasted value\n",
    "# for 9 weather variables in each time stamp\n",
    "# The meaning of the 9 variables that NAM predicts:\n",
    "# NAM_FIELDS = [\n",
    "#     \"accum_precip\",\n",
    "#     \"dewpoint_temperature_2m\",\n",
    "#     \"downward_short_wave_flux\",\n",
    "#     \"relative_humidity_2m\",\n",
    "#     \"sea_level_pressure\",\n",
    "#     \"temperature_2m\",\n",
    "#     \"total_cloud_cover\",\n",
    "#     \"u_wind_80m\",\n",
    "#     \"v_wind_80m\",\n",
    "# ]\n",
    "\n",
    "all_nam_data = []\n",
    "for i in range(32):\n",
    "    data_file_name = \"nam_data-000\" + \"{:02d}\".format(i) + \"-of-00032.npy\"\n",
    "    if not path.exists(\"nam_data/\" + data_file_name):\n",
    "        # Download the part file\n",
    "        r = requests.get(\"http://storage.googleapis.com/gridmatic/roscoe/\" + data_file_name, stream=True)\n",
    "        with open(\"nam_data/\" + data_file_name, 'wb') as fin:\n",
    "            shutil.copyfileobj(r.raw, fin)\n",
    "    all_nam_data.extend(np.load(\"nam_data/\" + data_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A one-dimensional list of the timestamps for the NAM inputs (24049x1)\n",
    "# The dimension is the number of 1 HOUR INTERVALS in the time period\n",
    "nam_timestamps = pd.read_pickle(\"nam_timestamps.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas dataframe of the timestamps\n",
    "nam_df = pd.DataFrame({'timestamps': nam_timestamps, 'data_objects': all_nam_data});\n",
    "nam_df = nam_df.set_index('timestamps');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAM has a lot of hours/dates missing\n",
    "# We initially thought about cutting the ERCOT down to match, but decided to take advantage of the full NAM dataset\n",
    "# We therefore generate new NAM data so that it more closely matches the data from ERCOT\n",
    "# We generate hourly data, as a middle ground between days of missing NAM data and the 15-minute granularity of ERCOT\n",
    "\n",
    "# OLD SOLUTION WITH PADDING: Just copy one value into all subsequent consecutive hours that are missing\n",
    "# nam_padded_df = nam_df.resample('H').pad();\n",
    "# Now we just do linear interpolation, so this can be ignored\n",
    "# But this is useful to keep as a reference to an earlier technique\n",
    "\n",
    "# Create entries for the missing hours (initially with a value of NaN)\n",
    "nam_unfilled_df = nam_df.resample('H').asfreq();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def find_closest_value(adj_time, direction):\n",
    "    i = 0\n",
    "    adj_value = nam_unfilled_df.iloc[nam_unfilled_df.index.get_loc(adj_time)].iloc[0]\n",
    "    while (type(adj_value) is float and math.isnan(adj_value)):\n",
    "        i += 1\n",
    "        adj_time += direction * pd.Timedelta(hours=1)        \n",
    "        adj_value = nam_unfilled_df.iloc[nam_unfilled_df.index.get_loc(adj_time)].iloc[0]\n",
    "    return (adj_value,i)\n",
    "\n",
    "def generate_new_value(timestamp):\n",
    "    prev_value, prev_dist = find_closest_value(timestamp, -1)\n",
    "    next_value, next_dist = find_closest_value(timestamp, 1)\n",
    "    percentage = 1 if (prev_dist + next_dist) == 0 else prev_dist / (prev_dist + next_dist)\n",
    "    return np.add(prev_value, (np.subtract(next_value, prev_value) * percentage))\n",
    "\n",
    "def fill_in_missing_time(slice_of_df):\n",
    "    value = slice_of_df.iloc[0]\n",
    "    timestamp = slice_of_df.name\n",
    "    timestamp = pd.Timestamp(timestamp) if isinstance(timestamp, np.datetime64) else timestamp\n",
    "    new_val = generate_new_value(timestamp)\n",
    "    slice_of_df.iloc[0] = new_val\n",
    "    return slice_of_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the new data to fill the NaN values\n",
    "nam_interpolated_df = nam_unfilled_df.apply(fill_in_missing_time, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the final NAM dataset, with generated data\n",
    "# pprint(nam_interpolated_df[:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the NAM data into training and testing\n",
    "split_df = nam_interpolated_df.copy(deep=True)\n",
    "split_df['numerical_index'] = range(0, len(split_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nam_training_df = split_df[split_df['numerical_index'] % 25 != 0]\n",
    "nam_testing_df = split_df[split_df['numerical_index'] % 25 == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the numerical_index\n",
    "nam_training_df = nam_training_df.drop('numerical_index', 1)\n",
    "nam_testing_df = nam_testing_df.drop('numerical_index', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the training and testing datasets out\n",
    "# pprint(nam_testing_df[:25])\n",
    "# pprint(nam_training_df[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the split training and testing data to disk\n",
    "cleaned_data = \"cleaned_data/\"\n",
    "training_data_filename = \"nam_training_1_of_2.pkl\"\n",
    "testing_data_filename = \"nam_testing_2_of_2.pkl\"\n",
    "\n",
    "nam_training_df.to_pickle(cleaned_data + training_data_filename)\n",
    "nam_testing_df.to_pickle(cleaned_data + testing_data_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
